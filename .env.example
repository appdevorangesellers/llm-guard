# Application Configuration
APP_NAME=LLM Guard API
LOG_LEVEL=DEBUG
LOG_JSON=true
SCAN_FAIL_FAST=false
SCAN_PROMPT_TIMEOUT=30
SCAN_OUTPUT_TIMEOUT=30
LAZY_LOAD=true  # Enable lazy loading of models

# Authentication Token
AUTH_TOKEN=your_token_here  # Replace with your actual token

# Rate Limiting
RATE_LIMIT_ENABLED=true
RATE_LIMIT_LIMIT=100/minute  # Set rate limit to 100 requests per minute

# Model Path
MODEL_PATH=/path/to/your/models  # Replace with the actual path where your models are stored

# Tracing Configuration
TRACING_EXPORTER=console  # Set tracing exporter (could be 'console', 'otel_http', 'xray', etc.)
TRACING_OTEL_ENDPOINT=  # Optional: Set OpenTelemetry endpoint if required

# Metrics Configuration
METRICS_TYPE=prometheus  # Set the metrics exporter (could be 'prometheus', 'otel_http', etc.)
METRICS_ENDPOINT=  # Optional: Set metrics endpoint if required