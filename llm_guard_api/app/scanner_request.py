from pydantic import BaseModel
from typing import List
from enum import Enum

class PromptRequest(BaseModel):
    prompt: str
class CompetitorListRequest(BaseModel):
    competitors: List[str]
class BanTopicsRequest(BaseModel):
    prompt: str
    topics: list
    threshold: float
    
class AnonymizeRequest(BaseModel):
    prompt: str
    allowed_names: list
    hidden_names: list
    preamble: str

class AnonymizationRequest(BaseModel):
    """
    Represents a request body for anonymizing or deanonymizing the prompt and model output.
    
    Attributes:
        prompt (str): The sanitized prompt (which may contain placeholders).
        model_output (str): The model's output (which may contain placeholders).
    """
    prompt: str
    model_output: str


class BanTopicsRequest(BaseModel):
    """
    Request model for input scanning.
    Attributes:
        prompt (str): The input prompt to scan for banned topics.
        topics (list[str]): List of banned topics to check against.
        threshold (float): The risk threshold for classification.
    """
    prompt: str
    topics: list[str] = ["violence"]  # Default sensitive topic
    threshold: float = 0.5  # Default risk threshold

class BanTopicsOutputRequest(BaseModel):
    """
    Request model for output scanning.
    Attributes:
        prompt (str): The original input prompt.
        model_output (str): The generated model output to scan.
        topics (list[str]): List of banned topics to check against.
        threshold (float): The risk threshold for classification.
    """
    prompt: str
    model_output: str
    topics: list[str] = ["violence"]
    threshold: float = 0.5

class CodeScanRequest(BaseModel):
    """
    Represents a request body for scanning both the sanitized prompt and
    the model output to identify code snippets in banned programming languages.
    
    Attributes:
        prompt (str): The sanitized prompt.
        model_output (str): The model's output.
        languages (List[str]): A list of languages whose code snippets should be banned.
        is_blocked (bool): Whether to block or allow the listed languages.
    """
    prompt: str
    model_output: str
    languages: List[str]  # Languages to ban
    is_blocked: bool  # Whether to block the specified languages

class CompetitorScanRequest(BaseModel):
    """
    Represents a request body for scanning both the prompt and the model output 
    to identify mentions of competitors and handle them accordingly.
    
    Attributes:
        prompt (str): The input prompt for the model.
        model_output (str): The output generated by the model.
        competitors (List[str]): A list of competitors' names to be flagged or redacted.
        redact (bool): Whether to redact competitors' names or just flag them.
        threshold (float): A risk threshold for how confident the model should be before flagging competitors.
    """
    prompt: str
    model_output: str
    competitors: List[str]  # List of competitor names or variations
    redact: bool  # Whether to redact the competitor's name
    threshold: float = 0.5  # Threshold for entity detection confidence

class MatchType(str, Enum):
    STRING = "string" 
    WORD = "word"  
    FULL = "full"  
    PARTIAL = "partial" 

class BanSubstringsScanRequest(BaseModel):
    """
    Represents a request body for scanning both the prompt and model output 
    to identify banned substrings and handle them accordingly.
    
    Attributes:
        prompt (str): The input prompt for the model.
        model_output (str): The output generated by the model.
        substrings (List[str]): A list of banned substrings to be flagged or redacted.
        match_type (MatchType): The granularity of the matching - either 'string' or 'word'.
        case_sensitive (bool): Whether the matching should be case-sensitive.
        redact (bool): Whether to redact the banned substrings by replacing them with [REDACT].
        contains_all (bool): If True, all banned substrings must be present to trigger a flag.
    """
    prompt: str
    model_output: str
    substrings: List[str]  # List of banned substrings
    match_type: MatchType  # Match type: 'string' or 'word'
    case_sensitive: bool = False  # Whether the matching should be case-sensitive
    redact: bool = False  # Whether to replace substrings with [REDACT]
    contains_all: bool = False  # Whether all substrings must appear

class GibberishScanRequest(BaseModel):
    """
    Request model for scanning the output for gibberish content.
    
    Attributes:
        prompt (str): The input prompt for the language model.
        model_output (str): The output generated by the language model.
        match_type (MatchType): The granularity of the matching - either 'full' or 'partial'.
    """
    prompt: str
    model_output: str
    match_type: MatchType = MatchType.FULL  # Default to full text matching

class LanguageSameScanRequest(BaseModel):
    """
    Request model for checking if the prompt and output are in the same language.
    
    Attributes:
        prompt (str): The input prompt for the language model.
        model_output (str): The output generated by the language model.
    """
    prompt: str
    model_output: str

class RegexScanRequest(BaseModel):
    """
    Request model for scanning the output based on regex patterns.
    Attributes:
        prompt (str): The input prompt for the language model.
        model_output (str): The output generated by the language model.
        patterns (list): List of regex patterns to match.
        is_blocked (bool): Whether matching patterns should mark the output as invalid.
        match_type (str): Match type, either 'SEARCH' or 'FULL_MATCH'.
        redact (bool): Whether to redact the matched portions in the output.
    """
    prompt: str
    model_output: str
    patterns: list
    is_blocked: bool = True  # Default to blocking if pattern is found
    match_type: str = "SEARCH"  # Can be 'SEARCH' or 'FULL_MATCH'
    redact: bool = True  # Enable redaction of matched patterns

class SentimentScanRequest(BaseModel):
    """
    Request model for scanning the output based on sentiment analysis.
    
    Attributes:
        prompt (str): The input prompt for the language model.
        model_output (str): The output generated by the language model.
        threshold (float): The sentiment threshold for flagging negative sentiment.
    """
    prompt: str
    model_output: str
    threshold: float = 0  # Default to 0, meaning neutral or negative sentiments will be flagged

class ToxicityScanRequest(BaseModel):
    """
    Request model for scanning the output based on toxicity detection.
    
    Attributes:
        prompt (str): The input prompt for the language model.
        model_output (str): The output generated by the language model.
        threshold (float): The toxicity threshold for flagging toxic output.
        match_type (MatchType): The granularity of the matching, either full text or sentence level.
    """
    prompt: str
    model_output: str
    threshold: float = 0.5  # Default threshold for toxicity score
    match_type: MatchType = MatchType.FULL  # Default to sentence-level matching

class BiasScanRequest(BaseModel):
    """
    Request model for scanning the output for potential bias detection.
    
    Attributes:
        prompt (str): The input prompt for the language model.
        model_output (str): The output generated by the language model.
        threshold (float): The bias threshold for flagging biased output.
        match_type (MatchType): The granularity of the matching, either full text or sentence level.
    """
    prompt: str
    model_output: str
    threshold: float = 0.5  # Default threshold for bias score
    match_type: MatchType = MatchType.FULL  # Default to full text matching

class MaliciousScanRequest(BaseModel):
    prompt: str
    model_output: str
    threshold: float = 0.7  # Default threshold for malicious URL detection
class NoRefusalScanRequest(BaseModel):
    prompt: str
    model_output: str
    threshold: float = 0.5  # Default threshold for refusal detection
    match_type: MatchType = MatchType.FULL  # Default match type

class FactualConsistencyScanRequest(BaseModel):
    prompt: str
    model_output: str
    minimum_score: float = 0.7  # Minimum score threshold for factual consistency

class RelevanceScanRequest(BaseModel):
    prompt: str
    model_output: str
    threshold: float = 0.5  # Default threshold for relevance check

class SensitiveScanRequest(BaseModel):
    prompt: str
    model_output: str
    entity_types: list = ["PERSON", "EMAIL"]  # Default sensitive entity types to scan
    redact: bool = True  # Whether to redact sensitive information
